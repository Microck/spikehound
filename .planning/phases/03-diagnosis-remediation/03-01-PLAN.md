---
phase: 03-diagnosis-remediation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/agents/diagnosis_agent.py
  - src/models/diagnosis.py
  - src/llm/__init__.py
  - src/llm/foundry_client.py
  - tests/test_confidence_score.py
autonomous: true
user_setup:
  - service: azure-ai-foundry
    why: "Call GPT-4o for diagnosis synthesis"
    env_vars:
      - name: FOUNDRY_ENDPOINT
        source: "Azure AI Foundry project"
      - name: FOUNDRY_API_KEY
        source: "Azure AI Foundry project keys"
      - name: FOUNDRY_MODEL
        source: "Model deployment name (e.g., gpt-4o)"

must_haves:
  truths:
    - "Diagnosis Agent turns UnifiedFindings into a root cause hypothesis"
    - "Diagnosis output includes a confidence score from 0-100"
  artifacts:
    - path: "src/agents/diagnosis_agent.py"
      provides: "Diagnosis Agent implementation"
    - path: "src/models/diagnosis.py"
      provides: "Diagnosis structured output model"
    - path: "src/llm/foundry_client.py"
      provides: "LLM client wrapper (Foundry)"
  key_links:
    - from: "src/agents/diagnosis_agent.py"
      to: "src/llm/foundry_client.py"
      via: "LLM call"
      pattern: "FoundryClient"
---

<objective>
Add Diagnosis Agent that synthesizes findings into a structured root cause hypothesis with confidence.

Purpose: Convert raw investigation data into an actionable explanation for humans.
Output: Foundry LLM wrapper, diagnosis model, diagnosis agent + basic scoring tests.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/02-investigation-pipeline/02-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Foundry LLM client wrapper (minimal, mockable)</name>
  <files>src/llm/__init__.py, src/llm/foundry_client.py</files>
  <action>
Create `src/llm/foundry_client.py`:
- Add a thin, mockable wrapper class `FoundryClient` that calls the Azure OpenAI-compatible chat completions REST API.
- Use env vars:
  - `FOUNDRY_ENDPOINT` (example: `https://YOUR_RESOURCE_NAME.openai.azure.com`)
  - `FOUNDRY_MODEL` (Azure OpenAI deployment name)
  - `FOUNDRY_API_KEY`

Concrete API shape to implement (single place, no guessing elsewhere):
- `POST {FOUNDRY_ENDPOINT}/openai/deployments/{FOUNDRY_MODEL}/chat/completions?api-version=2024-06-01`
- Headers: `api-key: {FOUNDRY_API_KEY}`, `Content-Type: application/json`
- Body (minimum):
  - `messages: [{role: "system", content: ...}, {role: "user", content: ...}]`
  - `temperature: 0`
  - `max_tokens: 800`

Implement `complete_json(system_prompt, user_prompt, model_cls) -> dict`:
- Calls the endpoint above
- Extracts `choices[0].message.content`
- Parses it as JSON and validates with the provided `model_cls` (pydantic)
- If parsing/validation fails, raise a typed error (e.g., `FoundryResponseError`) so the agent can deterministically fall back

If any required env var is missing, raise `FoundryNotConfiguredError` (do not return partial data).
  </action>
  <verify>. .venv/bin/activate && PYTHONPATH=src python3 -c "from llm.foundry_client import FoundryClient; print('ok')"</verify>
  <done>Foundry client wrapper exists and centralizes all LLM-calling details in one file.</done>
</task>

<task type="auto">
  <name>Task 2: Create diagnosis model and confidence scoring helper</name>
  <files>src/models/diagnosis.py, tests/test_confidence_score.py</files>
  <action>
Create `src/models/diagnosis.py`:
- `RootCauseHypothesis`: `title`, `explanation`, `evidence` (list of strings)
- `Diagnosis`: `hypothesis`, `confidence` (0-100 int), `alternatives` (list of strings), `risks` (list of strings)
- Validation: confidence range.

Add `tests/test_confidence_score.py`:
- Ensure invalid confidence values are rejected.
  </action>
  <verify>. .venv/bin/activate && PYTHONPATH=src pytest -q</verify>
  <done>Diagnosis model exists with strict confidence validation.</done>
</task>

<task type="auto">
  <name>Task 3: Implement Diagnosis Agent (UnifiedFindings -> Diagnosis)</name>
  <files>src/agents/diagnosis_agent.py</files>
  <action>
Create `src/agents/diagnosis_agent.py`:
- `DiagnosisAgent.run(unified_findings) -> AgentResult`.
- Create a concise prompt that includes:
  - top cost drivers
  - key resource changes
  - similar incidents + resolutions
- Ask the model for JSON output matching the `Diagnosis` schema.
- Ask the model for JSON output matching the `Diagnosis` schema (use `FoundryClient.complete_json(...)` and validate with the pydantic model).

If LLM is not configured OR the Foundry call fails, return `status=degraded` but still produce a deterministic root-cause hypothesis so the phase goal is achievable offline.

Deterministic fallback rules (no randomness):
- Identify the top cost-driving resource id (from unified findings cost section). If missing, set `title="Unattributed cost increase"` and include evidence that cost driver was unavailable.
- If the top resource looks like a VM and unified findings indicate it is running AND there is evidence of missing auto-shutdown (explicit field or a finding string containing `auto-shutdown`/`shutdown`/`schedule` + "missing"/"disabled"), then:
  - `title="GPU VM left running without auto-shutdown"`
  - `confidence=80`
  - evidence includes: the cost driver resource id/name, current power state, and the missing shutdown indicator
- Else if VM is running unexpectedly (resource state says running and there is no recent deployment/change window indicating intentional usage), then:
  - `title="VM running longer than intended"`
  - `confidence=60`
- Else if there is a clear historical match (history agent returns a similar incident), then:
  - `title="Recurring cost anomaly similar to past incident"`
  - `confidence=55`
- Else:
  - `title="Compute usage increase"`
  - `confidence=40`

In all fallback cases, populate:
- `explanation` as 2-4 sentences referencing the specific findings that triggered the rule
- `evidence` with 3-6 bullet-like strings (no empty list)
- `alternatives` with at least 2 plausible alternatives (e.g., "legitimate workload spike", "misconfigured schedule", "resource leak")
- `risks` with at least 1 risk (e.g., "stopping VM may disrupt active jobs")
  </action>
  <verify>. .venv/bin/activate && PYTHONPATH=src python3 -c "from agents.diagnosis_agent import DiagnosisAgent; print('ok')"</verify>
  <done>Diagnosis Agent exists and returns an AgentResult with `data` matching the Diagnosis model.</done>
</task>

</tasks>

<verification>
- `pytest` passes
- (If Foundry configured) run Diagnosis Agent on a saved UnifiedFindings JSON and validate it produces a Diagnosis
</verification>

<success_criteria>
- Diagnosis agent produces root cause hypothesis + confidence score
</success_criteria>

<output>
After completion, create `.planning/phases/03-diagnosis-remediation/03-01-SUMMARY.md`
</output>
